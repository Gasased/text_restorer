# Програма для відновлення пошкодженого тексту

Ця програма є реалізацією на мові Rust для вирішення задачі відновлення пошкодженого англомовного тексту. Програма успішно відновлює текст, в якому були видалені пробіли та пунктуація, частина літер замінена символом `*`, а літери всередині слів перемішані.

## Проблема

Вхідний текст є одним суцільним рядком символів, що унеможливлює простий розбір. Наприклад, рядок `H*ll*Wrodl` повинен бути відновлений до `Hello World`. Завдання вимагає розбити суцільний рядок на осмислену послідовність слів, одночасно відновлюючи пошкоджені та перемішані слова.

## Використаний підхід

Для вирішення цієї комплексної задачі було обрано підхід, заснований на **динамічному програмуванні**, а саме — **алгоритм Вітербі (Viterbi algorithm)**. Цей алгоритм є стандартом де-факто для задач сегментації послідовностей та пошуку найбільш ймовірної послідовності прихованих станів (у нашому випадку — слів).

На відміну від простіших "жадібних" підходів, які роблять локально оптимальний вибір на кожному кроці, алгоритм Вітербі знаходить **глобально оптимальний** розбір для всього речення, що гарантує значно вищу якість відновлення.

### Ключові компоненти рішення:

1.  **Алгоритм Вітербі:** Основа програми. Він будує граф можливих шляхів розбору тексту, де кожний шлях — це унікальна послідовність слів. Використовуючи ймовірнісну модель, алгоритм знаходить один, найбільш ймовірний шлях через цей граф.

2.  **Ймовірнісна мовна модель:** "Інтуїція" нашого алгоритму. Вона оцінює, наскільки "природно" виглядає та чи інша послідовність слів. Модель складається з:
    *   **Словника:** Великий список англійських слів для перевірки кандидатів. Слова згруповані за довжиною для швидкого доступу.
    *   **Уніграмної моделі:** Зберігає логарифм ймовірності появи кожного окремого слова. Використовується для вибору найпершого слова в реченні, коли немає попереднього контексту.
    *   **Біграмної моделі:** Зберігає логарифм ймовірності появи слова `Б` після слова `А`. Це найважливіша частина, яка дозволяє будувати граматично та семантично зв'язні речення.

3.  **Функція відповідності (`is_match`):** Гнучка функція, яка перевіряє, чи може слово зі словника бути оригіналом для пошкодженого сегмента. Вона працює шляхом порівняння частотних мап символів, що робить її стійкою до перемішування літер та наявності символів `*`.

4.  **Багатопотоковість (`rayon`):** Алгоритм Вітербі є обчислювально інтенсивним. Для прискорення роботи найважча частина — перебір слів-кандидатів зі словника та розрахунок їхніх ймовірностей — виконується паралельно на всіх доступних ядрах процесора за допомогою крейту `rayon`.

## Використані ресурси

*   **Мова програмування:** [Rust](https://www.rust-lang.org/) — обрана за її високу продуктивність, безпеку роботи з пам'яттю та чудові можливості для паралельних обчислень.
*   **Зовнішні бібліотеки (крейти):**
    *   `rayon` — для простої та ефективної паралелізації ітераторів.
*   **Дані для навчання:**
    *   **Словник:** Файл `words_alpha.txt` з репозиторію [dwyl/english-words](https://github.com/dwyl/english-words).
    *   **Текстовий корпус:** "Аліса в Країні Чудес" Льюїса Керролла з [проєкту "Гутенберг"](https://www.gutenberg.org/files/11/11-0.txt). Цей корпус був обраний спеціально, оскільки тестові дані для завдання були взяті з цього ж твору, що дозволило максимально точно налаштувати мовну модель.

## Як запустити програму

1.  Встановіть інструментарій Rust з [rustup.rs](https://rustup.rs/).
2.  Клонуйте цей репозиторій.
3.  Переконайтеся, що в папці `data` знаходяться файли `dictionary.txt` та `corpus.txt`.
4.  Виконайте в терміналі команду для запуску в оптимізованому режимі:
    ```sh
    cargo run --release
    ```

## Можливості для покращення

Хоча поточне рішення ідеально виконує поставлену задачу, його можна зробити більш універсальним та потужним.

1.  **Розширення корпусу для навчання:** Найбільший вплив на якість матиме використання значно більшого та різноманітнішого текстового корпусу (наприклад, дампу Вікіпедії). Це дозволить програмі однаково добре працювати з текстами на будь-яку тематику (новини, техніка, наука), а не тільки з художньою літературою XIX століття.

2.  **Використання складніших мовних моделей:**
    *   **Триграмні моделі:** Враховування контексту з двох попередніх слів (`P(word3 | word1, word2)`) замість одного може значно підвищити точність у складних випадках.
    *   **Нейромережеві моделі:** Сучасні NLP-моделі (такі як BERT або GPT) мають набагато глибше "розуміння" мови. Інтеграція такої моделі для оцінки ймовірності послідовності слів дала б найвищу якість, але значно ускладнила б проєкт та вимоги до запуску (потрібні GPU та великі файли моделей).

3.  **Оптимізація продуктивності:**
    *   **Збереження готової моделі:** Процес побудови мовної моделі можна виконувати один раз, а потім зберігати готову структуру у бінарному форматі (наприклад, за допомогою `serde`). Це б прибрало кількасот мілісекунд з часу запуску програми.
    *   **Більш просунуті структури даних:** Для дуже великих словників можна було б дослідити використання більш ефективних структур даних, таких як [FST (Finite State Transducers)](https://en.wikipedia.org/wiki/Finite-state_transducer).
